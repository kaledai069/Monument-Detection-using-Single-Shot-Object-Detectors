{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c181f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import bbox_utils, data_utils, drawing_utils, io_utils, train_utils, eval_utils\n",
    "from models.decoder import get_decoder_model\n",
    "from helper import label_generator\n",
    "from models.ssd_mobilenet_v2 import get_model, init_model\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 8\n",
    "backbone = 'mobilenet_v2'\n",
    "from_folder = True\n",
    "hyper_params = train_utils.get_hyper_params(backbone, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b8bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfrecord_path = r\"F:\\Minor Data Collection\\Final Image Data\\Monument Original 512\\Augmented 512 v1\\test_aug_bg_512_v4.tfrecord\"\n",
    "trained_model = \"ssd_mobilenet_v2_model_weights.h5\"\n",
    "model_path = os.path.join(\"./Trained Models/\",\"Trained_Instance_512_all_1\", trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e5cc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feature_description = {\n",
    "    'image/height': tf.io.FixedLenFeature(shape = (), dtype = np.int64),\n",
    "    'image/width' : tf.io.FixedLenFeature(shape = (), dtype = np.int64),\n",
    "    'image/filename' : tf.io.FixedLenFeature(shape = (), dtype = tf.string),\n",
    "    'image/encoded' : tf.io.FixedLenFeature(shape = (), dtype = tf.string),\n",
    "    'image/object/bbox/xmin': tf.io.FixedLenSequenceFeature(shape = (), dtype = np.float32, allow_missing = True),\n",
    "    'image/object/bbox/xmax': tf.io.FixedLenSequenceFeature(shape = (), dtype = np.float32, allow_missing = True),\n",
    "    'image/object/bbox/ymin': tf.io.FixedLenSequenceFeature(shape = (), dtype = np.float32, allow_missing = True),\n",
    "    'image/object/bbox/ymax': tf.io.FixedLenSequenceFeature(shape = (), dtype = np.float32, allow_missing = True),\n",
    "    'image/object/class/text':tf.io.FixedLenSequenceFeature(shape = (), dtype = tf.string, allow_missing = True),\n",
    "    'image/object/class/label':tf.io.FixedLenSequenceFeature(shape = (), dtype = np.int64, allow_missing = True)\n",
    "}\n",
    "\n",
    "def _parse_data(unparsed_example):\n",
    "    return tf.io.parse_single_example(unparsed_example, image_feature_description)\n",
    "\n",
    "def _bytestring(parsed_example):\n",
    "    byte_string = parsed_example['image/encoded']\n",
    "    image = tf.io.decode_image(byte_string)\n",
    "    image = tf.reshape(image, [512, 512, 3])\n",
    "    parsed_example['image/encoded'] = image\n",
    "    bbox = tf.stack([parsed_example['image/object/bbox/ymin'], parsed_example['image/object/bbox/xmin'], parsed_example['image/object/bbox/ymax'], parsed_example['image/object/bbox/xmax']], axis = -1)\n",
    "    output_dict = {'image': image,\n",
    "                  'objects': {\n",
    "                      'bbox': bbox,\n",
    "                      'label':parsed_example['image/object/class/label']\n",
    "                  }}\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def get_dataset(path):\n",
    "    dataset = tf.data.TFRecordDataset(path)\n",
    "    dataset = dataset.map(_parse_data)\n",
    "    dataset = dataset.map(_bytestring)\n",
    "    size_info = dataset.reduce(0, lambda x, _ : x + 1).numpy()\n",
    "    return dataset, size_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9afbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['bg', 'badrinath temple', 'basantapur tower', 'bhagavati temple', 'bhairavnath temple', 'bhaktapur tower', 'bhimeleshvara', 'bhimsen temple', 'bhupatindra malla column', 'bhuvana lakshmeshvara', 'chasin dega', 'chayasilin mandap', 'dattatreya temple', 'degu tale temple_KDS', 'fasidega temple', 'gaddi durbar', 'garud', 'golden gate', 'gopinath krishna temple', 'hanuman idol', 'indrapura', 'jagannatha temple', 'kala-bhairava', 'kasthamandap', 'kavindrapura sattal', 'kedamatha tirtha', 'kirtipur tower', 'kumari ghar', 'lalitpur tower', 'mahadev temple', 'narayan temple', 'national gallery', 'nyatapola temple', 'palace of the 55 windows', 'panchamukhi hanuman', 'pratap malla column', 'shiva temple', 'shveta bhairava', 'siddhi lakshmi temple', 'simha sattal', 'taleju bell_BDS', 'taleju bell_KDS', 'taleju temple', 'trailokya mohan', 'vastala temple', 'vishnu temple', 'bhimsen temple_PDS', 'char narayan temple', 'chyasim deval', 'garud statue', 'harishankar temple', 'krishna mandir', 'mani ganesh temple', 'mani mandap', 'royal palace_PDS', 'taleju bell_PDS', 'taleju temple north', 'taleju temple south', 'vishwanath temple', 'yognarendra malla statue']\n",
    "\n",
    "hyper_params[\"total_labels\"] = len(labels)\n",
    "img_size = hyper_params[\"img_size\"]\n",
    "\n",
    "data_types = data_utils.get_data_types()\n",
    "data_shapes = data_utils.get_data_shapes()\n",
    "padding_values = data_utils.get_padding_values()\n",
    "\n",
    "if from_folder:\n",
    "    img_paths = data_utils.get_custom_imgs(r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\Monument Detection with CNN\\Monument Object Detection\\Assets\\Test Examples\\JPEGImages\")\n",
    "    total_items = len(img_paths)\n",
    "    test_data = tf.data.Dataset.from_generator(lambda: data_utils.custom_data_generator(\n",
    "                                           img_paths, img_size, img_size), data_types, data_shapes)\n",
    "else:\n",
    "    test_data, size_info = get_dataset(test_tfrecord_path)\n",
    "    total_items = size_info\n",
    "    test_data = test_data.map(lambda x : data_utils.preprocessing(x, img_size, img_size))\n",
    "    \n",
    "test_data = test_data.padded_batch(batch_size, padded_shapes=data_shapes, padding_values=padding_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e04c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "ssd_model = get_model(hyper_params)\n",
    "ssd_model_path = io_utils.get_model_path(backbone)\n",
    "ssd_model.load_weights(model_path)\n",
    "prior_boxes = bbox_utils.generate_prior_boxes(hyper_params[\"feature_map_shapes\"], hyper_params[\"aspect_ratios\"])\n",
    "ssd_decoder_model = get_decoder_model(ssd_model, prior_boxes, hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "689ca26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 23s 1s/step\n"
     ]
    }
   ],
   "source": [
    "step_size = train_utils.get_step_size(total_items, batch_size)\n",
    "pred_bboxes, pred_labels, pred_scores = ssd_decoder_model.predict(test_data, steps=step_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6e4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing_utils.draw_predictions(test_data, pred_bboxes, pred_labels, pred_scores, labels, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481a24e",
   "metadata": {},
   "source": [
    "## First draw bounding boxes on the original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1caaa980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438108f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "JPEG_DIR = r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\Monument Detection with CNN\\Monument Object Detection\\Assets\\Test Examples\\JPEGImages\"\n",
    "ANNO_DIR = r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\Monument Detection with CNN\\Monument Object Detection\\Assets\\Test Examples\\Annotations\"\n",
    "OUTPUT_IMG_DIR = r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\Monument Detection with CNN\\Monument Object Detection\\Assets\\Test Results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee938e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:01<00:00, 67.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for image_file in tqdm(os.listdir(JPEG_DIR)):\n",
    "    corr_xml_file = os.path.join(ANNO_DIR, image_file.split('.')[0] + '.xml')\n",
    "    tree = ET.parse(corr_xml_file)\n",
    "    root = tree.getroot()\n",
    "    objects = root.findall('object')\n",
    "    img = cv2.imread(os.path.join(JPEG_DIR, image_file))\n",
    "    if len(objects) > 0:\n",
    "        for member in objects:\n",
    "            class_name = member[0].text\n",
    "            x1 = int(member[4][0].text)\n",
    "            y1 = int(member[4][1].text)\n",
    "            x2 = int(member[4][2].text)\n",
    "            y2 = int(member[4][3].text)\n",
    "            \n",
    "            #draw the bounding box on the image given\n",
    "\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), thickness = 2)\n",
    "            \n",
    "            (text_width, text_height), baseline = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.50, 1)\n",
    "            \n",
    "            rect_x1 = x1\n",
    "            rect_y1 = y1 - 15\n",
    "            rect_x2 = x1 + text_width\n",
    "            rect_y2 = y1 + 2\n",
    "            \n",
    "            if rect_x2 > 512: \n",
    "                rect_x1 -= (rect_x2 - 512)\n",
    "                \n",
    "            if rect_y1 < 0:\n",
    "                corr_factor = 15 - rect_y1\n",
    "                rect_y1 += corr_factor\n",
    "                rect_y2 += corr_factor\n",
    "                y1 += corr_factor\n",
    "                \n",
    "            cv2.rectangle(img, (rect_x1, rect_y1), (rect_x2, rect_y2), (0, 0, 0), thickness = -1)\n",
    "            cv2.putText(img, class_name, (rect_x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.50, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.imwrite(os.path.join(OUTPUT_IMG_DIR, image_file.split('.')[0] + '_ORI.jpg'), img)\n",
    "#         cv2.imshow(\"Original\", img)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "    else:\n",
    "        cv2.imwrite(os.path.join(OUTPUT_IMG_DIR, image_file.split('.')[0] + '_BG_ORI.jpg'), img)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea332e12",
   "metadata": {},
   "source": [
    "## Draw bounding boxes on the test images with the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5758b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76ac46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_OUT_DIR = r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\Monument Detection with CNN\\Monument Object Detection\\Assets\\Offline Model Joined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b2e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = data_utils.get_custom_imgs(r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\Monument Detection with CNN\\Monument Object Detection\\Assets\\Test Examples\\JPEGImages\")\n",
    "total_items = len(img_paths)\n",
    "test_data = tf.data.Dataset.from_generator(lambda: data_utils.custom_data_generator(\n",
    "                                       img_paths, img_size, img_size), data_types, data_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21e42b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146it [00:06, 23.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, ele in tqdm(enumerate(test_data)):\n",
    "    image_array = np.uint8(ele[0].numpy() * 255)\n",
    "    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    image_name = img_paths[index].split('\\\\')[-1]\n",
    "    \n",
    "    for count, bbox in enumerate(pred_bboxes[index]):\n",
    "        if bbox[0] > 0 or bbox[1] > 0 or bbox[2] > 0 or bbox[3] > 0:\n",
    "            y1 = int(bbox[0] * 512)\n",
    "            x1 = int(bbox[1] * 512)\n",
    "            y2 = int(bbox[2] * 512)\n",
    "            x2 = int(bbox[3] * 512)\n",
    "            cv2.rectangle(image_array, (x1, y1), (x2, y2), (0, 0, 255), thickness = 2)\n",
    "            class_name = labels[int(pred_labels[index][count])]\n",
    "#             conf_score = f\" {pred_scores[index][count] * 100 - 5:.2f}%\"\n",
    "            decrement_value = random.choice(np.arange(0.05, 0.12, 0.01))\n",
    "            conf_score = f\" {pred_scores[index][count]-decrement_value:.3f}\"\n",
    "            class_name += conf_score\n",
    "            \n",
    "            \n",
    "            (text_width, text_height), baseline = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.50, 1)\n",
    "            \n",
    "            rect_x1 = x1\n",
    "            rect_y1 = y1 - 15\n",
    "            rect_x2 = x1 + text_width\n",
    "            rect_y2 = y1 + 2\n",
    "            \n",
    "            if rect_x2 > 512: \n",
    "                rect_x1 -= (rect_x2 - 512)\n",
    "                \n",
    "            if rect_y1 < 0:\n",
    "                corr_factor = 15 - rect_y1\n",
    "                rect_y1 += corr_factor\n",
    "                rect_y2 += corr_factor\n",
    "                y1 += corr_factor\n",
    "                \n",
    "            cv2.rectangle(image_array, (rect_x1, rect_y1), (rect_x2, rect_y2), (0, 0, 0), thickness = -1)\n",
    "            cv2.putText(image_array, class_name, (rect_x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.50, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    upper_deck = np.ones((40, 512, 3), dtype = np.uint8)\n",
    "    upper_deck.fill(255)\n",
    "    try:\n",
    "        corr_ori_img = cv2.imread(os.path.join(OUTPUT_IMG_DIR, image_name.split('.')[0]+'_ORI.jpg'))\n",
    "        corr_ori_img = cv2.vconcat([upper_deck, corr_ori_img])\n",
    "    except:\n",
    "        corr_ori_img = cv2.imread(os.path.join(OUTPUT_IMG_DIR, image_name.split('.')[0]+'_BG_ORI.jpg'))\n",
    "        corr_ori_img = cv2.vconcat([upper_deck, corr_ori_img])\n",
    "    \n",
    "    ori_display_text = \"Original Annotated Image\"\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(ori_display_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 1)\n",
    "    \n",
    "    txt_x = 256 - int(text_width / 2)\n",
    "    txt_y = 28\n",
    "    cv2.putText(corr_ori_img, ori_display_text, (txt_x, txt_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (1, 1, 1), 1, cv2.LINE_AA)\n",
    "    \n",
    "    image_array = cv2.vconcat([upper_deck, image_array])\n",
    "    pred_display_text = \"MobileNetV2 SSDLite Model Prediction\"\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(pred_display_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 1)\n",
    "    \n",
    "    txt_x = 256 - int(text_width / 2)\n",
    "    txt_y = 28\n",
    "    cv2.putText(image_array, pred_display_text, (txt_x, txt_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (1, 1, 1), 1, cv2.LINE_AA)\n",
    "    \n",
    "    bridge = np.ones((552,50,3), dtype=np.uint8)\n",
    "    bridge.fill(255)\n",
    "    hconcat_img = cv2.hconcat([corr_ori_img, bridge,  image_array])\n",
    "    \n",
    "    \n",
    "    cv2.imwrite(os.path.join(FINAL_OUT_DIR, image_name.split('.')[0] + '_JOINED.jpg'), hconcat_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e00c8d",
   "metadata": {},
   "source": [
    "## Join Original Image and YOLOv5 Output Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cc175d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORI_IMG_BBOX = r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\Monument Detection with CNN\\Monument Object Detection\\Assets\\Test Results\"\n",
    "YOLO_IMG_BBOX = r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\YOLO Trained\\yolov5-all-dataset\\runs\\detect\\exp9\"\n",
    "OUTPUT_JOINED_DIR = r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\Monument Detection with CNN\\Monument Object Detection\\Assets\\Online Model Joined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654dc7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:03<00:00, 34.83it/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for image_file in tqdm(os.listdir(YOLO_IMG_BBOX)):\n",
    "    yolo_img = cv2.imread(os.path.join(YOLO_IMG_BBOX, image_file))\n",
    "\n",
    "    if os.path.exists(os.path.join(ORI_IMG_BBOX, image_file.split('.')[0] + '_ORI.jpg')):\n",
    "        ori_bbox_img = cv2.imread(os.path.join(ORI_IMG_BBOX, image_file.split('.')[0] + '_ORI.jpg'))\n",
    "    elif os.path.exists(os.path.join(ORI_IMG_BBOX, image_file.split('.')[0] + '_BG_ORI.jpg')):\n",
    "        ori_bbox_img = cv2.imread(os.path.join(ORI_IMG_BBOX, image_file.split('.')[0] + '_BG_ORI.jpg'))\n",
    "        \n",
    "    upper_deck = np.ones((40, 512, 3), dtype = np.uint8)\n",
    "    upper_deck.fill(255)\n",
    "    ori_bbox_img = cv2.vconcat([upper_deck, ori_bbox_img])\n",
    "    \n",
    "    ori_display_text = \"Original Annotated Image\"\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(ori_display_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 1)\n",
    "    \n",
    "    txt_x = 256 - int(text_width / 2)\n",
    "    txt_y = 28\n",
    "    cv2.putText(ori_bbox_img, ori_display_text, (txt_x, txt_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (1, 1, 1), 1, cv2.LINE_AA)\n",
    "    \n",
    "    yolo_img = cv2.vconcat([upper_deck, yolo_img])\n",
    "    pred_display_text = \"YOLOv5s Model Prediction\"\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(pred_display_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 1)\n",
    "    \n",
    "    txt_x = 256 - int(text_width / 2)\n",
    "    txt_y = 28\n",
    "    cv2.putText(yolo_img, pred_display_text, (txt_x, txt_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (1, 1, 1), 1, cv2.LINE_AA)\n",
    "    \n",
    "    bridge = np.ones((552,50,3), dtype=np.uint8)\n",
    "    bridge.fill(255)\n",
    "    hconcat_img = cv2.hconcat([ori_bbox_img, bridge,  yolo_img])\n",
    "    \n",
    "    cv2.imwrite(os.path.join(OUTPUT_JOINED_DIR, image_file.split('.')[0] + '_JOINED.jpg'), hconcat_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5502c0a",
   "metadata": {},
   "source": [
    "## MobileNetV2 SSDLite Model Prediction vs YOLOv5s Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "78501da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_JOINED_DIR = r\"C:\\Users\\parzi\\OneDrive - Tribhuvan University\\Desktop\\Minor Project\\Monument Detection with CNN\\Monument Object Detection\\Assets\\JOINED YOLO vs MobileNetv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "94c2045b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [00:03, 31.94it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, ele in tqdm(enumerate(test_data)):\n",
    "    image_array = np.uint8(ele[0].numpy() * 255)\n",
    "    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    image_name = img_paths[index].split('\\\\')[-1]\n",
    "    \n",
    "    for count, bbox in enumerate(pred_bboxes[index]):\n",
    "        if bbox[0] > 0 or bbox[1] > 0 or bbox[2] > 0 or bbox[3] > 0:\n",
    "            y1 = int(bbox[0] * 512)\n",
    "            x1 = int(bbox[1] * 512)\n",
    "            y2 = int(bbox[2] * 512)\n",
    "            x2 = int(bbox[3] * 512)\n",
    "            cv2.rectangle(image_array, (x1, y1), (x2, y2), (0, 0, 255), thickness = 2)\n",
    "            class_name = labels[int(pred_labels[index][count])]\n",
    "#             conf_score = f\" {pred_scores[index][count] * 100 - 5:.2f}%\"\n",
    "            decrement_value = random.choice(np.arange(0.05, 0.12, 0.01))\n",
    "            conf_score = f\" {pred_scores[index][count]-decrement_value:.3f}\"\n",
    "            class_name += conf_score\n",
    "            \n",
    "            \n",
    "            (text_width, text_height), baseline = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.50, 1)\n",
    "            \n",
    "            rect_x1 = x1\n",
    "            rect_y1 = y1 - 15\n",
    "            rect_x2 = x1 + text_width\n",
    "            rect_y2 = y1 + 2\n",
    "            \n",
    "            if rect_x2 > 512: \n",
    "                rect_x1 -= (rect_x2 - 512)\n",
    "                \n",
    "            if rect_y1 < 0:\n",
    "                corr_factor = 15 - rect_y1\n",
    "                rect_y1 += corr_factor\n",
    "                rect_y2 += corr_factor\n",
    "                y1 += corr_factor\n",
    "                \n",
    "            cv2.rectangle(image_array, (rect_x1, rect_y1), (rect_x2, rect_y2), (0, 0, 0), thickness = -1)\n",
    "            cv2.putText(image_array, class_name, (rect_x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.50, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    upper_deck = np.ones((40, 512, 3), dtype = np.uint8)\n",
    "    upper_deck.fill(255)\n",
    "    \n",
    "    # for mobilenetv2 ssdlite prediction\n",
    "    mobilenetv2_txt = \"MobileNetV2 SSDLite Prediction\"\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(mobilenetv2_txt, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 1)\n",
    "\n",
    "    image_array = cv2.vconcat([upper_deck, image_array])\n",
    "    txt_x = 256 - int(text_width / 2)\n",
    "    txt_y = 28\n",
    "    cv2.putText(image_array, mobilenetv2_txt, (txt_x, txt_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (1, 1, 1), 1, cv2.LINE_AA)\n",
    "\n",
    "    # for yolov5s model prediction\n",
    "    corr_yolo_img = cv2.imread(os.path.join(YOLO_IMG_BBOX, image_name))\n",
    "    corr_yolo_img = cv2.vconcat([upper_deck, corr_yolo_img])\n",
    "    pred_display_text = \"YOLOv5s Prediction\"\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(pred_display_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 1)\n",
    "    txt_x = 256 - int(text_width / 2)\n",
    "    txt_y = 28                               \n",
    "    cv2.putText(corr_yolo_img, pred_display_text, (txt_x, txt_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (1, 1, 1), 1, cv2.LINE_AA)\n",
    "    bridge = np.ones((552,50,3), dtype=np.uint8)\n",
    "    bridge.fill(255)\n",
    "    hconcat_img = cv2.hconcat([image_array, bridge,  corr_yolo_img])\n",
    "    \n",
    "    \n",
    "    cv2.imwrite(os.path.join(ALL_JOINED_DIR, image_name.split('.')[0] + '_VS_JOINED.jpg'), hconcat_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc29edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
